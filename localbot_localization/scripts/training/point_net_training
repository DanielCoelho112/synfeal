#!/usr/bin/env python3

import sys
import argparse
import yaml
import torch
import torch.nn.parallel
import torch.optim as optim
import torch.nn as nn
import torch.utils.data
from localbot_localization.src.models.pointnet import PointNet, feature_transform_regularizer
from localbot_localization.src.dataset import LocalBotDataset
from localbot_localization.src.loss_functions import BetaLoss, DynamicLoss
from localbot_localization.src.utilities import process_pose
import numpy as np
from datetime import datetime
import matplotlib.pyplot as plt
import os
from colorama import Fore



def main():
    
    parser = argparse.ArgumentParser(description='PointNet training')
    
    parser.add_argument('-fn', '--folder_name', type=str, required=True, help='folder name')
    parser.add_argument('-mn', '--name', type=str, required=True, help='model name')
    parser.add_argument('-train_set', '--training_set', type=str, required=True, help='Name of the training set')
    parser.add_argument('-test_set', '--testing_set', type=str, required=True, help='Name of the testing set')
    parser.add_argument('-n_epochs', '--n_epochs', type=int, required=True, help='Number of epochs')
    parser.add_argument('-batch_size', '--batch_size', type=int, required=True, help='Batch size')
    parser.add_argument('-loss', '--loss_function', type=str, default='DynamicLoss()', help='Type of loss function. [DynamicLoss(), BetaLoss(B)]')
    parser.add_argument('-lr', '--learning_rate', type=float, default=0.001, help='Learning rate value')
    parser.add_argument('-lr_step_size', '--lr_step_size', type=int, default=20, help='Step size of the learning rate decay')
    parser.add_argument('-lr_gamma', '--lr_gamma', type=float, default=0.5, help='Decay of the learning rate after step size')
    
    
    arglist = [x for x in sys.argv[1:] if not x.startswith('__')]
    args = vars(parser.parse_args(args=arglist))
    
    folder_name = f'{os.environ["HOME"]}/models/localbot/{args["folder_name"]}'
    model_name = f'{folder_name}/{args["name"]}.pth'
    
    if not os.path.exists(folder_name):
        print(f'Creating folder {folder_name}')
        os.makedirs(folder_name)
    else:
        print(f'{Fore.RED} {folder_name} already exists... Aborting training {Fore.RESET}')
        exit(0)

    ## Load the dataset
    train_dataset = LocalBotDataset(path_seq=args['training_set'])
    test_dataset = LocalBotDataset(path_seq=args['testing_set'])
    
    # check if the datasets are validated
    
    config_train = train_dataset.getConfig()
    config_test = test_dataset.getConfig()
    if not config_train['is_valid']:
        print(f'{Fore.RED} {train_dataset.seq} is not valid!')
        exit(0)
    if not config_test['is_valid']:
        print(f'{Fore.RED} {test_dataset.seq} is not valid!')
        exit(0)
    
    
    
    
    ## Pytorch data loader
    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True,  num_workers=2)
    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=args['batch_size'], shuffle=False,  num_workers=2)
    # Num_workers tells the data loader instance how many sub-processes to use for data loading. If the num_worker is zero (default) the GPU has to wait for CPU to load data.

    ## Build the model
    model = PointNet() # TODO: change this to work with different models.

    ## Optmizer and Loss

    #criterion = BetaLoss()
    criterion = eval(args['loss_function'])

    # Add all params for optimization
    param_list = [{'params': model.parameters()}]
    if isinstance(criterion, DynamicLoss):
        # Add sx and sq from loss function to optimizer params
        param_list.append({'params': criterion.parameters()})

    optimizer = optim.Adam(params = param_list, lr=args['learning_rate']) # the most common optimizer in DL
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args['lr_step_size'], gamma=args['lr_gamma']) # variable learning rate. After 5 epochs, the lr decays 0.5


    print(torch.cuda.is_available())
    #device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu") # if cuda is available use CUDA
   
    #model.to(device)
    #criterion.to(device)
    model.cuda()
    criterion.cuda()

    n_epochs = args['n_epochs']
    train_losses = np.zeros(n_epochs)
    test_losses = np.zeros(n_epochs)

    for epoch in range(n_epochs):
        t0 = datetime.now()
        train_loss = []
        scheduler.step() # here we are telling the scheduler that: n_epochs += 1
        for data in train_dataloader:
            points, target = data

            points = points.transpose(2, 1) # 3xN which is what our network is expecting
            points, target = points.cuda(), target.cuda() # move data into GPU
            
            optimizer.zero_grad() # Clears the gradients of all optimized tensors (always needed in the beginning of the training loop)
            
            model = model.train() # Sets the module in training mode. For example, the dropout module can only be use in training mode.
            
            pred, trans, trans_feat = model(points) # our model outputs the pose, and the transformations used
            
            pred = process_pose(pred)
                
            loss = criterion(pred, target)
            
            ## L1 REGULARIZATION
            # if feature_transform:
            #     loss += feature_transform_regularizer(trans_feat) * 0.001 ## Regularization! --> Prevent overfitting by adding something to the cost function. The simpler the model the lower the cost function
            
            
            loss.backward() # Computes the gradient of current tensor w.r.t. graph leaves.
            optimizer.step() # Performs a single optimization step (parameter update).
            
            train_loss.append(loss.item())
        train_loss = np.mean(train_loss)
        
        
        
        test_loss=[]
        for data in test_dataloader:
            points, target = data
            points = points.transpose(2, 1) # 3xN which is what our network is expecting
            points, target = points.cuda(), target.cuda() # move data into GPU
            model = model.eval() # Sets the module in evaluation mode.

            pred, _, _ = model(points)
            
            pred = process_pose(pred)
            
            loss = criterion(pred, target)
            
            test_loss.append(loss.item())
        test_loss = np.mean(test_loss)
        

        # save losses
        train_losses[epoch] = train_loss
        test_losses[epoch] = test_loss
        
        dt = datetime.now() - t0
        print(f'epoch {epoch+1}/{n_epochs}, train_loss: {train_loss:.4f}, test_loss: {test_loss:.4f}, duration: {dt}')
            
    
    plt.plot(train_losses, label='train loss')
    plt.plot(test_losses, label='test loss')
    plt.legend()
    #plt.show()
    
    
    plt.savefig(f'{folder_name}/losses.png')
    torch.save(model.state_dict(), model_name)
    
    dt_now = datetime.now() # current date and time
    config = {'user'          : os.environ["USER"],
              'date'          : dt_now.strftime("%d/%m/%Y, %H:%M:%S"),
              'train_set'     : args['training_set'],
              'test_set'      : args['testing_set'],
              'n_epochs'      : args['n_epochs'],
              'batch_size'    : args['batch_size'],
              'loss'          : args['loss_function'],
              'learning_rate' : args['learning_rate'],
              'lr_step_size'  : args['lr_step_size'],
              'lr_gamma'      : args['lr_gamma'],
              'init_model'    : 'PointNet()'}
    
    with open(f'{folder_name}/config.yaml', 'w') as file:
        yaml.dump(config, file)
    
if __name__ == "__main__":
    main()

