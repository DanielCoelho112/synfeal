#!/usr/bin/env python3

import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from colorama import Fore, Style
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime  # to track the time each epoch takes
import argparse
import sys
import os
import yaml
from yaml.loader import SafeLoader
from colorama import Fore
import pickle

from localbot_localization.src.dataset import LocalBotDataset
from localbot_localization.src.loss_functions import BetaLoss, DynamicLoss
from localbot_localization.src.utilities import process_pose
from localbot_localization.src.models.posenet import PoseNetGoogleNet, PoseNetResNet
from localbot_localization.src.models.poselstm import PoseLSTM
from localbot_localization.src.models.hourglass import Hourglass, HourglassBatch
from localbot_localization.src.torch_utilities import summarizeModel, resumeTraining
from torchvision import transforms

def main():
    parser = argparse.ArgumentParser(description='PointNet training')
    parser.add_argument('-v', '--visualize', action='store_true')
    parser.add_argument('-c', '--cuda', action='store_true')
    parser.add_argument('-gpu', '--gpu', type=int, default=0)
    parser.add_argument('-sm', '--summarize_model', action='store_true')
    parser.add_argument('-fn', '--folder_name', type=str, required=True, help='folder name')
    parser.add_argument('-mn', '--model_name', type=str, required=True, help='model name')
    parser.add_argument('-train_set', '--training_set', type=str, required=True, help='Name of the training set')
    parser.add_argument('-test_set', '--testing_set', type=str, required=True, help='Name of the testing set')
    parser.add_argument('-n_epochs', '--n_epochs', type=int, required=True, help='Number of epochs')
    parser.add_argument('-ss', '--save_step', type=int, default=10, help='Each X epochs, the parameters of the model are save')
    parser.add_argument('-batch_size', '--batch_size', type=int, required=True, help='Batch size')
    parser.add_argument('-loss', '--loss_function', type=str, default='DynamicLoss()',
                        help='Type of loss function. [DynamicLoss(), BetaLoss(B)]')
    parser.add_argument('-lr', '--learning_rate', type=float, default=1e-4, help='Learning rate value')
    parser.add_argument('-lr_step_size', '--lr_step_size', type=int, default=20,
                        help='Step size of the learning rate decay')
    parser.add_argument('-lr_gamma', '--lr_gamma', type=float, default=0.5,
                        help='Decay of the learning rate after step size')
    parser.add_argument('-wd', '--weight_decay', type=float, default=0, help='L2 regularizer')
    parser.add_argument('-im', '--init_model', type=str, default='CNNDepth()',
                        help='Decay of the learning rate after step size')
    parser.add_argument('-cs', '--crop_size', type=int, default=299,
                        help='Size of the random and center crop. For PoseNet/LSTM use 299, for Hourglass use 224')

    arglist = [x for x in sys.argv[1:] if not x.startswith('__')]
    args = vars(parser.parse_args(args=arglist))

    if args['cuda']:
        if not torch.cuda.is_available():
            raise ValueError('Cuda is not available.')
        else:
            print(f'{Fore.GREEN} Cuda is available {Fore.RESET}')
            torch.cuda.set_device(args['gpu'])
            
    folder_name = f'{os.environ["HOME"]}/models/localbot/{args["folder_name"]}'
    model_name = f'{folder_name}/{args["model_name"]}.pth'
    
    config_stats = LocalBotDataset(path_seq=args['training_set']).getConfig()['statistics']
    rgb_mean = [config_stats['R']['mean'], config_stats['G']['mean'], config_stats['B']['mean']]
    rgb_std = [config_stats['R']['std'], config_stats['G']['std'], config_stats['B']['std']]
    
    #depth_mean = config_stats['D']['mean']
    #depth_std = config_stats['D']['std']
    
    
    rgb_transform_train = transforms.Compose([
        transforms.Resize(args['crop_size'] + 1),
        transforms.RandomCrop(args['crop_size']),
        transforms.ToTensor(),
        transforms.Normalize(rgb_mean, rgb_std)
    ])
    
    rgb_transform_test = transforms.Compose([
        transforms.Resize(args['crop_size'] + 1),
        transforms.CenterCrop(args['crop_size']),
        transforms.ToTensor(),
        transforms.Normalize(rgb_mean, rgb_std)
    ])
    
    
    print(f'rgb_transform_test is: {rgb_transform_test.transforms}')
    
    train_dataset = LocalBotDataset(path_seq=args['training_set'], rgb_transform = rgb_transform_train, depth_transform = None, inputs=['rgb_image'])
    test_dataset = LocalBotDataset(path_seq=args['testing_set'], rgb_transform = rgb_transform_test, depth_transform = None, inputs=['rgb_image'])

    # check if the datasets are validated
    config_train = train_dataset.getConfig()
    config_test = test_dataset.getConfig()
    if not config_train['is_valid']:
        print(f'{Fore.RED} {train_dataset.seq} is not valid!')
        exit(0)
    if not config_test['is_valid']:
        print(f'{Fore.RED} {test_dataset.seq} is not valid!')
        exit(0)

    batch_size = args['batch_size']
    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)


    # start training, resume training, or abandon training?
    if os.path.exists(folder_name):
        print(f'Folder already exists! Do you want to resume training?')
        ans = input("YES/no")
        
        if ans.lower() in ['', 'yes','y']:
            start_epoch, train_losses, test_losses, model = resumeTraining(folder_name)
        else:
            print(f'{Fore.RED} Terminating training... {Fore.RESET}')
            exit(0)
    else:
        print(f'Model Folder not found: {folder_name}. Starting from sratch.')
        os.makedirs(folder_name)
        model = eval(args['init_model'])
        train_losses = []
        test_losses = []
        start_epoch = 0
        
    # convert transforms to pickle files
    with open(f'{folder_name}/rgb_transform_train.pickle', 'wb') as handle:
        pickle.dump(rgb_transform_train, handle, protocol=pickle.HIGHEST_PROTOCOL)
    with open(f'{folder_name}/rgb_transform_test.pickle', 'wb') as handle:
        pickle.dump(rgb_transform_test, handle, protocol=pickle.HIGHEST_PROTOCOL)
    
    
    if args['summarize_model']:
        summarizeModel(model, train_dataset[0][1])
        
    # ----------------------------------
    # Define loss function
    # ----------------------------------
    criterion = eval(args['loss_function'])
    
    # Add all params for optimization
    param_list = [{'params': model.parameters()}]
    if isinstance(criterion, DynamicLoss):
        # Add sx and sq from loss function to optimizer params
        param_list.append({'params': criterion.parameters()})

    optimizer = optim.Adam(params = param_list, lr=args['learning_rate'], weight_decay=args['weight_decay']) # the most common optimizer in DL
    # weight decay is the L2 regularizer!!

    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args['lr_step_size'], gamma=args['lr_gamma'])

    # ----------------------------------
    # Training
    # ----------------------------------
    
    stored_test_loss_epoch = np.inf
    
    n_epochs = args['n_epochs']

    first_time = True
    save_step = args['save_step'] # when to store the model

    if args['cuda']:
        model.cuda()  # Send model to GPU
        criterion.cuda()  # Send criterion to GPU
        
    for epoch in range(start_epoch, n_epochs):
        t0 = datetime.now()
        train_losses_per_batch = []

        for rgb_image, target_pose in train_loader:  # iterate batches
            
            if args['cuda']:
                rgb_image, target_pose = rgb_image.cuda(), target_pose.cuda()  # move data into GPU
                
            optimizer.zero_grad()  # Clears the gradients of all optimized tensors (always needed in the beginning of the training loop)

            model = model.train()  # Sets the module in training mode. For example, the dropout module can only be use in training mode.

            if model.aux_logits:
            
                pose_aux1, pose_aux2, pose = model(rgb_image)  # our model outputs the pose, and the transformations used
                
                pose_aux1  = process_pose(pose_aux1)
                pose_aux2  = process_pose(pose_aux2)
                pose = process_pose(pose)
                
                loss_aux1 = criterion(pose_aux1, target_pose)
                loss_aux2 = criterion(pose_aux2, target_pose)
                loss_pose = criterion(pose, target_pose)
                
                loss = loss_pose + 0.3 * (loss_aux1 + loss_aux2) # https://arxiv.org/abs/1409.4842
            
            else:
                pose = model(rgb_image)
                pose = process_pose(pose)
                loss = criterion(pose, target_pose)
                

    
            loss.backward()  # Computes the gradient of current tensor w.r.t. graph leaves.
            optimizer.step()  # Performs a single optimization step (parameter update).

            train_losses_per_batch.append(loss.item())

        train_loss_epoch = float(np.mean(train_losses_per_batch))

        test_losses_per_batch = []
        for rgb_image, target_pose in test_loader:
            if args['cuda']:
                rgb_image, target_pose = rgb_image.cuda(), target_pose.cuda()  # move data into GPU

            model = model.eval()  # Sets the module in evaluation mode.

            predicted_pose = model(rgb_image)  # our model outputs the pose, and the transformations used
            predicted_pose = process_pose(predicted_pose)

            loss = criterion(predicted_pose, target_pose)

            test_losses_per_batch.append(loss.item())

        test_loss_epoch = float(np.mean(test_losses_per_batch))

        # save losses
        train_losses.append(train_loss_epoch)
        test_losses.append(test_loss_epoch)
        
        dt = datetime.now() - t0
        print(
            f'epoch {epoch + 1}/{n_epochs}, train_loss: {train_loss_epoch:.4f}, test_loss: {test_loss_epoch:.4f}, duration: {dt}')
        
        
        scheduler.step()

        # ----------------------
        # Visualization
        # ----------------------
        if args['visualize']:
            if first_time:
                hanle_train_plot = plt.plot(train_losses, label='train loss')
                hanle_test_plot = plt.plot(test_losses, label='test loss')
                first_time = False
                plt.legend()
            else:
                hanle_train_plot[0].set_data(range(0,len(train_losses)), train_losses)
                hanle_test_plot[0].set_data(range(0,len(test_losses)), test_losses)
                axis = plt.gca()
                axis.relim(), axis.autoscale_view() # resize axis

            plt.draw()
            plt.waitforbuttonpress(0.01)
                   
        # ------------------
        # Saving of Model
        # ------------------
        if (epoch+1)%save_step == 0:
            
            # only save if test loss is lower than the previous saved!
            if test_loss_epoch < stored_test_loss_epoch:
                
                plt.figure()
                plt.plot(train_losses, label='train loss')
                plt.plot(test_losses, label='test loss')
                plt.legend()
                plt.savefig(f'{folder_name}/losses.png')
                torch.save(model.state_dict(), model_name)

                dt_now = datetime.now()  # current date and time
                config = {'user': os.environ["USER"],
                        'date': dt_now.strftime("%d/%m/%Y, %H:%M:%S"),
                        'train_set': args['training_set'],
                        'test_set': args['testing_set'],
                        'max_n_epochs': args['n_epochs'],
                        'epoch': epoch +1,
                        'batch_size': args['batch_size'],
                        'loss': args['loss_function'],
                        'learning_rate': args['learning_rate'],
                        'lr_step_size': args['lr_step_size'],
                        'lr_gamma': args['lr_gamma'],
                        'init_model': args['init_model'],
                        'train_losses': train_losses,
                        'test_losses': test_losses,
                        'weight_decay' : args['weight_decay']}

                with open(f'{folder_name}/config.yaml', 'w') as file:
                    yaml.dump(config, file)

                print(f'Saved model at epoch {epoch+1}')
                stored_test_loss_epoch = test_loss_epoch
            else:
                print('Current model is no better than the stored model. Ignoring saving.')
        

if __name__ == "__main__":
    main()
