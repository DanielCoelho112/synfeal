#!/usr/bin/env python3

import cv2
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from colorama import Fore, Style
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime  # to track the time each epoch takes
import argparse
import sys
import os
import yaml
from yaml.loader import SafeLoader
from colorama import Fore

from localbot_localization.src.dataset import LocalBotDataset
from localbot_localization.src.loss_functions import BetaLoss, DynamicLoss
from localbot_localization.src.utilities import process_pose
from localbot_localization.src.models.depthnet import CNNDepth, CNNDepthLow, CNNDepthBatch, CNNDepthDropout, CNNDepthBatchLowLeaky
from localbot_localization.src.torch_utilities import summarizeModel, resumeTraining

def main():
    parser = argparse.ArgumentParser(description='PointNet training')
    parser.add_argument('-v', '--visualize', action='store_true')
    parser.add_argument('-c', '--cuda', action='store_true')
    parser.add_argument('-sm', '--summarize_model', action='store_true')
    parser.add_argument('-fn', '--folder_name', type=str, required=True, help='folder name')
    parser.add_argument('-mn', '--model_name', type=str, required=True, help='model name')
    parser.add_argument('-train_set', '--training_set', type=str, required=True, help='Name of the training set')
    parser.add_argument('-test_set', '--testing_set', type=str, required=True, help='Name of the testing set')
    parser.add_argument('-n_epochs', '--n_epochs', type=int, required=True, help='Number of epochs')
    parser.add_argument('-ss', '--save_step', type=int, default=10, help='Each X epochs, the parameters of the model are save')
    parser.add_argument('-batch_size', '--batch_size', type=int, required=True, help='Batch size')
    parser.add_argument('-loss', '--loss_function', type=str, default='DynamicLoss()',
                        help='Type of loss function. [DynamicLoss(), BetaLoss(B)]')
    parser.add_argument('-lr', '--learning_rate', type=float, default=1e-4, help='Learning rate value')
    parser.add_argument('-lr_step_size', '--lr_step_size', type=int, default=20,
                        help='Step size of the learning rate decay')
    parser.add_argument('-lr_gamma', '--lr_gamma', type=float, default=0.5,
                        help='Decay of the learning rate after step size')
    parser.add_argument('-im', '--init_model', type=str, default='CNNDepth()',
                        help='Decay of the learning rate after step size')

    arglist = [x for x in sys.argv[1:] if not x.startswith('__')]
    args = vars(parser.parse_args(args=arglist))

    if args['cuda']:
        if not torch.cuda.is_available():
            raise ValueError('Cuda is not available.')
        else:
            print(f'{Fore.GREEN} Cuda is available {Fore.RESET}')
            
    folder_name = f'{os.environ["HOME"]}/models/localbot/{args["folder_name"]}'
    model_name = f'{folder_name}/{args["model_name"]}.pth'
    
    
    train_dataset = LocalBotDataset(path_seq=args['training_set'])
    test_dataset = LocalBotDataset(path_seq=args['testing_set'])

    # check if the datasets are validated
    config_train = train_dataset.getConfig()
    config_test = test_dataset.getConfig()
    if not config_train['is_valid']:
        print(f'{Fore.RED} {train_dataset.seq} is not valid!')
        exit(0)
    if not config_test['is_valid']:
        print(f'{Fore.RED} {test_dataset.seq} is not valid!')
        exit(0)

    batch_size = args['batch_size']
    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)


    # start training, resume training, or abandon training?
    if os.path.exists(folder_name):
        print(f'Folder already exists! Do you want to resume training?')
        ans = input("YES/no")
        
        if ans.lower() in ['', 'yes','y']:
            start_epoch, train_losses, test_losses, model = resumeTraining(folder_name)
        else:
            print(f'{Fore.RED} Terminating training... {Fore.RESET}')
            exit(0)
    else:
        print(f'Model Folder not found: {folder_name}. Starting from sratch.')
        os.makedirs(folder_name)
        model = eval(args['init_model'])
        train_losses = []
        test_losses = []
        start_epoch = 0
    
    if args['summarize_model']:
        summarizeModel(model, train_dataset[0][1])
        
    # ----------------------------------
    # Define loss function
    # ----------------------------------
    criterion = eval(args['loss_function'])
    
    # Add all params for optimization
    param_list = [{'params': model.parameters()}]
    if isinstance(criterion, DynamicLoss):
        # Add sx and sq from loss function to optimizer params
        param_list.append({'params': criterion.parameters()})

    optimizer = optim.Adam(params = param_list, lr=args['learning_rate'], weight_decay=1e-5) # the most common optimizer in DL
    # weight decay is the L2 regularizer!!

    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args['lr_step_size'], gamma=args['lr_gamma'])

    # ----------------------------------
    # Training
    # ----------------------------------

    n_epochs = args['n_epochs']

    first_time = True
    save_step = args['save_step'] # when to store the model

    if args['cuda']:
        model.cuda()  # Send model to GPU
        criterion.cuda()  # Send criterion to GPU
        
    for epoch in range(start_epoch, n_epochs):
        t0 = datetime.now()
        train_losses_per_batch = []

        for points, depth_image, rgb_image, target_pose in train_loader:  # iterate batches
            
            if args['cuda']:
                depth_image, target_pose = depth_image.cuda(), target_pose.cuda()  # move data into GPU
                
            optimizer.zero_grad()  # Clears the gradients of all optimized tensors (always needed in the beginning of the training loop)

            model = model.train()  # Sets the module in training mode. For example, the dropout module can only be use in training mode.

            predicted_pose = model(depth_image)  # our model outputs the pose, and the transformations used

            predicted_pose = process_pose(predicted_pose)

            loss = criterion(predicted_pose, target_pose)

            loss.backward()  # Computes the gradient of current tensor w.r.t. graph leaves.
            optimizer.step()  # Performs a single optimization step (parameter update).

            train_losses_per_batch.append(loss.item())

        train_loss_epoch = float(np.mean(train_losses_per_batch))

        test_losses_per_batch = []
        for points, depth_image, rgb_image, target_pose in test_loader:
            depth_image, target_pose = depth_image.cuda(), target_pose.cuda()  # move data into GPU

            model = model.eval()  # Sets the module in evaluation mode.

            predicted_pose = model(depth_image)  # our model outputs the pose, and the transformations used
            predicted_pose = process_pose(predicted_pose)

            loss = criterion(predicted_pose, target_pose)

            test_losses_per_batch.append(loss.item())

        test_loss_epoch = float(np.mean(test_losses_per_batch))

        # save losses
        train_losses.append(train_loss_epoch)
        test_losses.append(test_loss_epoch)
        
        dt = datetime.now() - t0
        print(
            f'epoch {epoch + 1}/{n_epochs}, train_loss: {train_loss_epoch:.4f}, test_loss: {test_loss_epoch:.4f}, duration: {dt}')
        
        
        scheduler.step()

        # ----------------------
        # Visualization
        # ----------------------
        if args['visualize']:
            if first_time:
                hanle_train_plot = plt.plot(train_losses, label='train loss')
                hanle_test_plot = plt.plot(test_losses, label='test loss')
                first_time = False
                plt.legend()
            else:
                hanle_train_plot[0].set_data(range(0,len(train_losses)), train_losses)
                hanle_test_plot[0].set_data(range(0,len(test_losses)), test_losses)
                axis = plt.gca()
                axis.relim(), axis.autoscale_view() # resize axis

            plt.draw()
            plt.waitforbuttonpress(0.01)
            
        # ------------------
        # Saving of Model
        # ------------------
        if (epoch+1)%save_step == 0:
            
            plt.savefig(f'{folder_name}/losses.png')
            torch.save(model.state_dict(), model_name)

            dt_now = datetime.now()  # current date and time
            config = {'user': os.environ["USER"],
                      'date': dt_now.strftime("%d/%m/%Y, %H:%M:%S"),
                      'train_set': args['training_set'],
                      'test_set': args['testing_set'],
                      'max_n_epochs': args['n_epochs'],
                      'epoch': epoch +1,
                      'batch_size': args['batch_size'],
                      'loss': args['loss_function'],
                      'learning_rate': args['learning_rate'],
                      'lr_step_size': args['lr_step_size'],
                      'lr_gamma': args['lr_gamma'],
                      'init_model': args['init_model'],
                      'train_losses': train_losses,
                      'test_losses': test_losses}

            with open(f'{folder_name}/config.yaml', 'w') as file:
                yaml.dump(config, file)

            print(f'Saved model at epoch {epoch+1}')

if __name__ == "__main__":
    main()
